### 卷积算子优化

卷积运算是深度学习中常用的操作之一，但是由于其计算量较大，在大型神经网络中可能成为性能瓶颈，导致训练时间过长。因此，为了提高卷积运算的计算效率，需要通过并行编程等技术手段进行优化。

使用`im2col`+`gemm`对卷积算子进行优化，着重对`gemm`调优。

主要工作：

- 使用cuda编写im2col部分代码，把输入图像转化成二维矩阵，用于下一步的矩阵乘法。

- 使用cuda编写gemm代码，同时对边界情况进行处理以适应不同形状的输入数据。

- 使用preload，共享内存，解决bank冲突，双缓冲技术优化手段，针对题目中不同形状的输入进行特定的线程块配置，尽可能提高卷积算子性能。

	

`奖项：先导杯计算应用大奖赛华东赛区一等奖`

### Paddle 第四期黑客松：为 Paddle 优化 Lerp OP 在 GPU 上的性能

飞桨的Lerp OP是用Eigen实现的。Eigen中的Broadcast过于复杂全面，严重拖慢了飞桨Lerp OP的速度，性能不足。可以基于飞桨内部的Broadcast Kernel实现良好的优化效果。

主要工作：

- 调研pytorch中lerp实现逻辑，寻找逻辑上优化的可能。
- 使用飞桨性能较好的BroadcastKernel替换Eigen中的Broadcast，编写相应代码。
- 使用飞桨内置OP Benchmark进行算子性能测试，对比优化前后的OP性能情况。

[PR link](https://github.com/PaddlePaddle/Paddle/pull/53154)

