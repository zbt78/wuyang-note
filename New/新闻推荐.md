## 资源汇总

- News Recommendation: 包含多种深度学习模型([here](https://github.com/yusanshi/news-recommendation))
- microsoft/recommenders: 微软的推荐模型[(here)](https://github.com/microsoft/recommenders)

## 知识图谱

## Self-Attention

> query, key, value

query 、 key & value 的概念其实来源于推荐系统。

基本原理是：给定一个 query，计算query 与 key 的相关性，然后根据query 与 key 的相关性去找到最合适的 value。

举个例子：在电影推荐中。query 是某个人对电影的喜好信息（比如兴趣点、年龄、性别等）、key 是电影的类型（喜剧、年代等）、value 就是待推荐的电影。

q代表query，用来match其他单词；
k代表key，用来被query匹配的；
v代表要被抽取出来的information。

## 4-24

在序列化推荐中,循环神经网络,卷积神经网络,自注意力和图神经网络是使用非常广泛的.

对比电子商务,电影推荐,新闻推荐有几个独特的特点:

- 生命周期短, 和电子商务推荐不同,它们的项目能够活跃几个月甚至几年的时间,但是新闻文章在几天内就失效了. 因此在对项目节点进行建模的时候,不能简单的使用Id嵌入
- 具有候选感知用户建模的新闻推荐(Fine-grained candidate-aware user modeling)(深入了解下)
- 新闻文章中包含丰富的文本信息,与NLP之间有密切联系,这是其他领域的推荐系统所不具备的

在模型训练过程中可以引入对比学习的思想,对正样本进行负采样,最大化两者的差距,能够利用更多的负样本信息。



## 4-25

写了论文草稿

## 4-26

要从用户的历史记录中学习到长期兴趣和短期兴趣，但是用户的记录数量是非常稀疏的，很难学习到用户的兴趣

***在推荐的时候，随机加入一些用户并没有交互过的news，假设他们之间有关系(待验证)。*** 在新闻推荐中，用户的id信息不再那么重要，要从新闻标题和内容中提取有效信息。必须要借助NLP模型来学习新闻的深度表示。 主要技术可以使用multi-head self-attention，additive attention networks，KNN，这些方法不仅不传统的手工标注特征的方式效率高，不需要繁重的工作，通常效果也会更好。



## 4-27

因为新闻中最丰富的就是文本信息，自然语言处理这部分必不可少，要从文本内容中得到有用信息。

首先新闻标题蕴含的信息很重要，对用户的点击行为通常起决定性作用。在对文本进行建模的时候，CNN是使用最频繁的结构，因为它能很好地捕获上下文信息。另外在新闻建模时，注意力机制也被用来选择重要的特征。并且Transformer在NLP中获得了巨大成功，在对新闻建模时可以使用类似于Transformer的架构来对新闻建模。或者是使用预训练的语言模型来增强新闻建模。

另外，可以把之前看的关于去偏的方法想办法运用到新闻推荐中，把这两方面内容结合起来。



## 4-28

self-attention用到无偏数据集集中,提高无偏数据集占的比重



## 5-4

去偏方向目前效果最好的一篇文章AutoDebias提出利用一个统一数据集通过元学习来解决样本不均衡+噪音标签的数据集的影响。而使用元学习来解决训练数据含有噪音的影响这一方法早在之前的一篇文章中被提出，AutoDebias更像是对这个方法进行了领域内的应用。这篇文章提出的方法可以很方便地嵌入到各种深度模型中，不需要调节超参，并且在样本不平衡，标签有噪音的数据集上取得了很好的表现。非常符合去偏的要求。利用现有的方法来进行组合应用或许能达到比较好的效果。





## 5-9

GCN在推荐系统中能获得不错的效果。其基本思路是将用户和商品之间相互依赖的关系建模成一个图，并且利用 GCN 模型学习到嵌入向量表示，以此预测用户对物品的偏好。在使用GCN时，可以将用户和商品视为连通图中的节点，边则代表用户与商品之间的交互行为。在训练期间，利用已有数据对图进行特征提取，然后使用监督学习方法进行优化，最终得到一个针对当前用户来推荐相关商品的模型。

GCN 在推荐系统中的应用主要有以下优点：

1. GCN可以对用户-物品交互图进行卷积操作，从而获得每个节点（物品）的表征向量。这些表征向量可以用于预测用户对物品的评分或者进行推荐。
2. 相比传统的推荐模型，如矩阵分解，GCN 能够捕捉到更多的用户或商品之间的关系，从而提高模型性能；
3. GCN可以将交互图中的用户、物品和场景进行建模，从而预测用户在不同场景下的行为和兴趣，进而进行推荐。在建模后还能与其他深度学习的方法进行结合，



## 5-11

```json
base: 
{'precision': array([0.28267544]), 
 'recall': array([0.31319141]), 
 'ndcg': array([0.39470864])}

miniconda:
	--useattention = True
	{'precision': array([0.28267544]), 
     'recall': array([0.31319141]), 
     'ndcg': array([0.39467599])}
	--useattention = False
	{'precision': array([0.26677632]), 
     'recall': array([0.31081902]), 
     'ndcg': array([0.37625791])}
```

LightGCN原文中在连接三层embedding时直接取平均值，还指出在技术上也可以学习层的组合系数{αk}，或者用注意网络参数化它们，但是在训练数据上学习α时并不会改进性能。我在组合系数α使用了一层注意力网络，对三层的权重分配进行学习，最后得到的结果要比之前提高1%。学习了Self-Attention 的原理与应用，Self-Attention 是深度学习中重要的思想之一，其主要作用是帮助模型学习输入序列中不同位置之间的相关性。在这个深度学习任务中，Self-Attention 扮演着非常重要的角色， Transformer 模型中 有对Self-Attention 的具体描述，其中使用多头注意力机制来增强模型的表达能力。通过仔细阅读论文和手动实现相关层次，我更加清楚地理解了这一知识点，并且为今后的深度学习任务打下了坚实的基础。





