
| **期刊**        | **NAME**        | **YEAR** |
| ----------- | ----------- | ----------- |
| x | [Bias and Unfairness of Collaborative Filtering Based Recommender Systems in MovieLens Dataset](https://ieeexplore.ieee.org/document/9808125/)       |2022        |
| x | [Blockbuster: A New Perspective on Popularity-bias in Recommender Systems](https://ieeexplore.ieee.org/document/9558877/)   | 2021        |
| x | [Topic Model-Based Recommender System for Longtailed Products Against Popularity Bias](https://ieeexplore.ieee.org/document/8923895/) | 2019 |
| x | [Examining Political Bias within YouTube Search and Recommendation Algorithms](https://ieeexplore.ieee.org/document/9660012/)| 2021|
| x | [A Graph-Based Approach for Mitigating Multi-Sided Exposure Bias in Recommender Systems.](https://dl.acm.org/doi/10.1145/3470948)| 2022|
| x | [A sampling approach to Debiasing the offline evaluation of recommender systems.](https://link.springer.com/article/10.1007/s10844-021-00651-y)| 2022|
| x | [Debiasing Learning for Membership Inference Attacks Against Recommender Systems](https://dl.acm.org/doi/10.1145/3534678.3539392)| 2022 |
|| [Diverse User Preference Elicitation with Multi-Armed Bandits](https://dl.acm.org/doi/10.1145/3437963.3441786)| 2021|
|| [Unbiased Learning to Rank in Feeds Recommendation](https://dl.acm.org/doi/10.1145/3437963.3441751)| 2021|
|| [Cross-Positional Attention for Debiasing Clicks](https://dl.acm.org/doi/10.1145/3442381.3450098)| 2021|
|| [Counterfactual Embedding Learning for Debiased Recommendation](https://ieeexplore.ieee.org/document/9643168/)| 2021|
| Yahoo & Coat | [Invariant Preference Learning for General Debiasing in Recommendation](https://www.thumedialab.com/papers/InvPref.pdf)| 2021|
| Yahoo & Coat | [Interpolative Distillation for Unifying Biased and Debiased Recommendation](https://dl.acm.org/doi/10.1145/3477495.3532002)| 
| Yahoo & Coat | [Debiased Recommendation with User Feature Balancing](https://arxiv.org/abs/2201.06056v1)||
| Ciao & Epinions | [Causal Disentanglement with Network Information for Debiased Recommendations](https://arxiv.org/abs/2204.07221)|
| MovieLens | [Debiasing Learning for Membership Inference Attacks Against Recommender Systems](https://arxiv.org/abs/2206.12401v2)|
| Yahoo & MovieLens | [Debiasing Neighbor Aggregation for Graph Neural Network in Recommender Systems](https://arxiv.org/abs/2208.08847)|
| Yahoo & Coat | [AutoDebias: Learning to Debias for Recommendation](https://arxiv.org/abs/2105.04170)|
| MovieLens | [Debiasing Career Recommendations with Neural Fair Collaborative Filtering](http://jfoulds.informationsystems.umbc.edu/papers/2021/Islam%20(2021)%20-%20Debiasing%20Career%20Recommendations%20with%20Neural%20Fair%20Collaborative%20Filtering%20(WWW).pdf)|
| MovieLens | [It Is Different When Items Are Older: Debiasing Recommendations When Selection Bias and User Preferences Are Dynamic](https://arxiv.org/abs/2111.12481v1)|
| Yahoo & Coat| [Keeping Dataset Biases out of the Simulation](https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/huang-2020-keeping.pdf)|
| Yelp & MovieLens | [Self-Guided Learning to Denoise for Robust Recommendation](https://arxiv.org/abs/2204.06832)|
| Alexa & MovieLens & Yahoo| [Debiasing Neighbor Aggregation for Graph Neural Network in Recommender Systems](https://arxiv.org/abs/2208.08847)|
| Movie & Ciao| [Evolution of Popularity Bias: Empirical Study and Debiasing](https://arxiv.org/abs/2207.03372)|
| MovieLens & Yahoo & Coat| [Enhanced Doubly Robust Learning for Debiasing Post-click Conversion Rate Estimation](https://arxiv.org/abs/2105.13623)|
|| []|
|||


```toc

```

## Interpolative Distillation for Unifying Biased and Debiased Recommendation

现有的策略：
- data imputation：评估缺失数据的影响，减少选择偏差
- regularization：引入正则化减少推荐列表的偏差
- causal inference：两个经典的因果框架potential outcome、structural causal models


分成两种环境（现实环境bias & 非现实环境debias,并且Db >> Dd），计算对于一个特定的 user-item pair，属于每一种环境的概率：P(E | U, I)。在以往的推荐模型中 只考虑了一种环境，也就是现实环境，并没有考虑非现实环境。

用P(R | U,I,E) & P(E | U,I) 来估算 P(R | U,I):
 $P(R|U,I) = \sum_{E}^{}P(E|U,I,R)\cdot P(E|U,I)$

### 4.2
#### 4.2.1
使用 biased and debiased model 训练得到最优参数，然后预测出 rb_hat,rd_hat 以此来得到$P(R|U,I,E=eb)$ 和 $P(R|U,I,E=ed)$ 

#### 4.2.2
如果 given (u,i) pair 更可能属于哪一种环境，那么相应的 *rb_hat* or *rd_hat* 就会更接近真实的 *r* 。 *rb_hat* and *rd_hat* 都是在各自环境中表现的最好的算法得到的。*rd_hat* 是 $P(R|U,I,E=eb)$ 的估计期望值，越接近期望就意味着 *r* 有更大的可能性 comes from $P(R|U,I,E=eb)$ ，那么因此 $P(E=ed |U,I)$  应该会更大。$$
w_b=\frac{L_b\left( \overline{r_b},r \right) \gamma}{L_b\left( \overline{r_b},r \right) \gamma +L_{\mathrm{d}}\left( \overline{r_{\mathrm{d}}},r \right) \gamma}\,\,w_{\mathrm{d}}=\frac{L_{\mathrm{d}}\left( \overline{r_{\mathrm{d}}},r \right) \gamma}{L_b\left( \overline{r_b},r \right) \gamma +L_{\mathrm{d}}\left( \overline{r_{\mathrm{d}}},r \right) \gamma}
$$$\gamma$ is a negative value, 保证$L_b\left( \overline{r_b},r \right)$ 越小时，$w_b$ 越大，说明当 $\overline{r_b}$ 越接近 $r$ 时，$w_b$ 越大。


#### 4.2.3
$r_*$ 是 $P(R|U,I)$ 的期望，$r_*=w_b\overline{r_b} +w_d\overline{r_d}$  
然后在两个训练集（包括biased和debiased）上训练新的模型，新模型的损失函数是让训练结果和 $r_*$ 做比较，并不是观察的评分 $r$ 。

#### 4.2.4 纳入未观察到的数据
使用 $\mathcal{D} _n=\mathcal{U} \times \mathcal{I} -\mathcal{D} _b\cup \mathcal{D} _{\mathrm{d}}$ 来表示未观察的数据。
$w_b^{'}$ and $w_d^{'}$ 表示 $P(E=eb|U,I)$ and $P(E=ed|U,I)$ 的插值
$r_*^{'}=w_b^{'}\overline{r_b} +w_d^{'}\overline{r_d}$
$$
w_{b}^{'}=\frac{L_b\left( \overline{r_b},\overline{r} \right) \gamma _2}{L_b\left( \overline{r_b},\overline{r} \right) \gamma _2+L_{\mathrm{d}}\left( \overline{r_{\mathrm{d}}},\overline{r} \right) \gamma _2}\,\,w_{\mathrm{d}}^{'}=\frac{L_{\mathrm{d}}\left( \overline{r_{\mathrm{d}}},\overline{r} \right) \gamma _2}{L_b\left( \overline{r_b},\overline{r} \right) \gamma _2+L_{\mathrm{d}}\left( \overline{r_{\mathrm{d}}},\overline{r} \right) \gamma _2}
$$

$L_b\left( \overline{r_b},\overline{r} \right)$ and $L_d\left( \overline{r_d},\overline{r} \right)$ is the distances between prediction of teachers and student.
模型为 $f_S\left( u,i;\theta \right)$ 使用模型的预测值和 $r_*^{'}$ 来作为损失函数的参数。
对模型进行训练，当损失函数最小时到超参数 $\theta$ .

和其他算法的不同点：
- 同时利用了有偏模型和无偏模型
- 从两种模型中学习一个新的模型，能处理事实和反事实环境
- 使用了未观察的数据来训练 *student* 模型

### 5.3
受欢迎程度是用 *item* 在训练数据中出现的频次决定的。

在不受欢迎的 *item* 中，InterD 更相信 *debiased-teacher* ，而在受欢迎的 *item* 中，InterD则更信赖 *biased-teacher* 。

在 *top-20* 中，*AutoDebias* 推荐的项目 *less-popular* 占的比较多， *MF* 推荐的项目 *popular* 占的比较多
Yahoo:
![[Pasted image 20221031214223.png]]
![[Pasted image 20221031214259.png]]
![[Pasted image 20221031212502.png]]
Coat:
![[Pasted image 20221031214841.png]]
![[Pasted image 20221031214911.png]]
![[Pasted image 20221031214934.png]]

![[Pasted image 20221113112145.png]]


## InvariantPreferenceLearning

### Inroduction

1. 使用 random logging policy 来采集 uniform data，然后来监督model在有偏数据集上的训练
2. 要找到由不能观察的干扰因子造成的潜在的偏差，然后得到普遍的去偏
3. 通过估计**伪环境标签**作为**代理**来捕获为观察到的干扰因子
4. 通过对抗学习的方式将不变偏好（即环境无关偏好）和变体偏好（即环境相关偏好）从可以观察的行为区分开来
5. **在所有环境中找到y_hat最接近y的环境**
6. Embedding就是用一个低维的向量表示一个物体，这个Embedding向量的性质是能使距离相近的向量对应的物体有相近的含义
7. 

使用*MovieLens-1M dataset*评价*Popularity bias*的表现
使用*Mind dataset*评价*Exposure bias*的表现


## Debiased Recommendation with User Feature Balancing


## Debiasing Learning for Membership Inference Attacks Against Recommender Systems

## Debiasing Career Recommendations with Neural Fair Collaborative Filtering

两种偏差修正来解决:
- (1)bias in the input embeddings due to the non-sensitive items, 
- and (2) bias in the prediction outputs due to the sensitive items. 

### 2.3 公平指标

#### 2.3.1 Differential Fairness
#### 2.3.2 Absolute Unfairness


### 3 neural fair collaborative filtering

使用CF***嵌入***来去除*性别偏差*：CF embeddings for users from each protected group

权重: $w^{'}=w-\left( w\cdot v_B \right) v_B$ 

女性用户偏差方向: $v_{f\mathrm{e}mal\mathrm{e}}=\frac{1}{n_f}\left( f_1+f_2+...+f_n \right)$ , $f_1,f_2,...,f_n$ are vectors for each female user

整体性别偏差向量: $v_B=\frac{v_{f\mathrm{e}mal\mathrm{e}}-v_{mal\mathrm{e}}}{\left\| v_{f\mathrm{e}mal\mathrm{e}}-v_{mal\mathrm{e}} \right\|}$

对用户向量进行去偏: $p_{u}^{'}=p_u-\left( p_u\cdot v_B \right) v_B$, $p_u$ is user vector.

上面这些是性别去偏。

然后用一个*fairness penalty*来去掉*sensetive items*中的人口统计偏差，*eg. more men
than women choose computer science careers.*


预训练，微调

What are no-sensetive items and sensetive items?

***结果在服务器上***



## It Is Different When Items Are Older: Debiasing Recommendations When Selection Bias and User Preferences Are Dynamic

本文目标：纠正动态情况下的偏见

每个项目的互动数量呈长尾分布，因为用户更可能与更受欢迎的项目进行互动；用户对他们喜欢的项目评价更频繁。

物品不会在很长时间内受欢迎；用户的喜欢也会随着时间变化

Yahoo and Coat 缺乏时间上的属性，不能作为数据集进行使用


## Leave No User Behind

It's the first to enforce preserving the unique user and item properties as the adversary to the process of learning how to recommend

把用户对项目的评论添加进来
NLP


## AutoDebias: Learning to Debias for Recommendation

explicit Yahoo:
![[Pasted image 20221101114559.png]]

explicit coat:
![[Pasted image 20221101115204.png]]

implicit Yahoo:
![[Pasted image 20221101145149.png]]

implicit coat:
![[Pasted image 20221101145338.png]]

simulation:
![[Pasted image 20221101145641.png]]



## Self-Guided Learning to Denoise for Robust Recommendation

### 1 Introduction
样本选择 和 样本重新加权   *sample selection methods* and *sample re-weighting methods*
样本选择严重依赖*样本分布*
样本重新加权也有如下限制：
- **Abandon of Hard Clean Interactions**: 一些干净的互动可能有很高的损失值，因此在重新加权的过程中被抛弃了 *aka.* hard yet clean interactions
- **Lack of Adaptivity and University**: 缺乏适应性和普遍性

SGDL分为两个时期：记忆期和自我主导学习期

在训练的早期阶段，被记忆的数据大多是容易和干净的交互，我们把它们作为去噪信号来收集，以指导接下来的去噪训练过程

推荐模型的记忆效应：the memorization effect of recommendation models

### 2 PROBLEM FORMULATION

### Preference learning from implicit feedback

assume that the interaction $y_{u,i}$ = {0,1} could represent hte user's **true** preferences

### Denoising implicit feedback for recommendations

### 3 METHODOLOGY

#### 3.1 Phase *one* : Memorization
我的问题：为什么记忆点之前的大部分被记忆的交互是干净的？


## E-commerce Recommendation with Weighted Expected Utility

Expected Utility 期望效用

### 3

#### 3.1 Expected Utility Hypothsis
设u为购买决策的效用函数，则用户j购买商品i的决策的期望效用EU可计算为:
$$
EU\left( i,j \right) =\sum_{o_{ij}\in O_{ij}}{u_j\left( o_{ij} \right) p_i\left( o_{ij} \right)}
$$
#### 3.2 Model of Utility
##### 3.2.1 Outcome Modeling for Consumer Satisfation

$$
o_{ij}=r_{ij}-\hat{r}_j
$$
$r_{ij}$ is the rating of item $i$ given by user $j$ ,
$\hat{r}_j$ is the reference rating point which user $j$ determines whether the purchase makes them statisfied or unsatisfied.



##### 3.2.2 Diminishing Marginal Utility
第一单位消费的商品或服务产生的效用比第二单位和随后的单位消费产生的效用要大，并且持续减少的数量更多

人们在做决定时往往以不同的方式看待得失，而每个人看待得失的情况也是不尽相同的：
$$
u_j\left( o_{ij} \right) =\begin{cases}
	\alpha _j\cdot \tanh \left( o_{ij} \right) ,&		if\,\,o_{ij}\geqslant 0\\
	\beta _j\cdot \tanh \left( o_{ij} \right) ,&		if\,\,o_{ij}<0\\
\end{cases}
$$

$$
P\left( o_{ij} \right) =P\left( r_{ij} \right) =\frac{\#\left( r_{ij},i \right)}{N}
$$
where N is the number of users who rated item i and #(ri j, i) is the count of users who give the same rating to i with user j.


#### 3.4Weighted Expected Utility
大多数情况下人们总是高估小概率事件而低估大概率事件。
为了模拟电子商务消费者行为的心理偏差，我们对电子商务推荐的EU假设进行了加权期望效用(WEU)的扩展,采用概率加权函数(PWF)来更好地解释消费者对可能购买结果概率的个人偏好：
$$
WEU\left( i,j \right) =\sum_{o_{ij}\in O_{ij}}{u_j\left( o_{ij} \right) \cdot w_j\left( p_i\left( o_{ij} \right) \right)}
$$




## Disentangling User Interest and Conformity for Recommendation with Causal Embedding

把流行性偏差分成两个部分来解决：一个是用户的兴趣，另一个是用户的从众（interest and conformity）

用户的点击行为可以分解为 interest & conformity 的相加：
$$
s_{ui}^{int}=\left< \boldsymbol{u}^{\left( int \right)},\boldsymbol{i}^{\left( int \right)} \right> s_{ui}^{con}=\left< \begin{array}{c}
	\boldsymbol{u}^{\left( con \right)},\boldsymbol{i}^{\left( con \right)}\\
\end{array} \right> 
\\ 
s_{ui}^{click}=s_{ui}^{int}+s_{ui}^{con}
$$

比如一个男人是否受欢迎，既要看他的长相，也要看他的脾气。外貌和脾气通常是独立的，人气是外貌和脾气的对撞机(外貌→人气←脾气)，从而点击可以看成是兴趣和从众的碰撞💥。

## A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data

Brige Strategy 桥策略
![[Pasted image 20221116151529.png]]
Sc是有偏数据集，St是无偏数据集，Sa是从整个数据集D中选出来的一部分数据，其中D包括Sc、St和unobserved data未观察到的数据。