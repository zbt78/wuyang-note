## Boston house price prediction: machine learning

### Introduction

#### What is in this essay?

我们将在这篇文章中讨论每种回归方法的优点和缺点。我们也会去了解数据科学家用来评估回归方法准确性的各种成本函数。通过回顾，你将对过去使用过的回归方法有更好的了解，并熟悉它们的缺陷。正如沃伦-巴菲特所说，"从你的错误中学习是好事。从别人的错误中学习更好"。学习其他人解决问题的方法的优点和缺点是改进或发展你自己的回归方法的一个重要部分。希望你能从这篇文章中得到启发，能够提高算法的准确性。

任务是在给定14个相关数据集的情况下预测房价的缺失值，如面积、位置、税收等（图1）我们的目标是开发一个模型，可以确定这些数值与房屋价格之间的关系。这个问题是一个典型的回归问题，因为我们希望程序的输出是一个数字值。

我们将研究如何用线性回归、多项式回归、随机森林回归和其他回归模型来解决这个问题。

#### 线性回归

可以说，线性回归是现有的最基本和最直接的回归模型。它只涉及简单的代数。不需要线性代数。这是如何做到的呢？从单变量线性回归开始可以使它更容易理解。考虑一下图2中所示的数据集。其中x代表自变量，y代表因变量。

假设图上的每个黑点代表训练集中的一个有序的配对（xn, yn）。现在问题来了。当x=a时，我们如何预测y（输出）值？解决这个问题的一个简单方法是画一条线L来拟合这些点并评估L(a)。
$$
L(x) = theta0 + theta1*x
     \\
L(a) = theta0 + theta1*a
$$
通过简单的加法，我们得到了一个预测值L(a)，但问题是我们不知道该线是否是最佳拟合线。为了确定该线的准确性，我们必须首先引入线性模型的成本函数，其中成本J满足公式

![image-20221204103449945](https://blog-img-zbt.oss-cn-beijing.aliyuncs.com/picture/wuyang/202212041034025.png)

J越大，我们的线性模型就越不准确。为了找到最佳拟合线，J必须被最小化。注意，当使用简单的微积分时，有可能找到准确的最佳拟合线。

然而，随着点的数量增加，计算将迅速变得更加复杂。因此，为了找到大数据集的最佳拟合线，引入了线性回归方法。

基本上，它通过少量的调整，使之朝着更好的直线大量的调整。这个过程被称为梯度下降。每次调整中的数值变化被称为学习率。显然，学习率越低，为了使成本J接近局部最小值，需要调整的次数就越多。然而，如果学习率设置得太高，J可能永远不会接近局部最小值。我们的工作就是要找出一个好的学习率。

我们必须注意的另一件事是，通过梯度下降，我们只能保证达到局部最小值（不一定是绝对最小值）。为了找到绝对最小值，我们可以尝试多个起点（theta0, theta1），然后比较结果。然而，即使我们经历了所有这些过程，我们仍然有可能无法达到绝对最小值。我们之所以做这些，只是为了达到一个满意的模型，而不是一个完美的模型。

当涉及到多变量的线性回归时，并没有很大的区别。唯一的区别是，我们有更多的X值集，所以方程必须改变。

![image-20221204103634266](https://blog-img-zbt.oss-cn-beijing.aliyuncs.com/picture/wuyang/202212041036300.png)

公式3显示了L(x)的矩阵方程，其中x1-xn代表n（在我们的例子中为14）个给定值，a0-an代表相应的系数。它也可以写成:

![image-20221204103708974](https://blog-img-zbt.oss-cn-beijing.aliyuncs.com/picture/wuyang/202212041037004.png)

在梯度下降过程中，我们需要同时更新每一个theta值。 请注意，在多变量线性回归中，可能存在更多的局部极值。这意味着我们必须比单变量线性回归做更多的试验。

线性回归的局限性：

- 每个x和y之间的关系应该是线性的，这样才能使线性回归发挥作用。然而，在实际世界中，大多数时候x和y值之间的关系不是简单的线性关系，这导致线性回归有一个相对较大的误差。
- 难以处理错误。传统的线性回归方法无法处理数据集中的错误。在现实生活中，当我们收集数据时，总会有误差。为了处理这个问题，我们开发了一种贝叶斯线性回归方法，它可以处理那些误差遵循正常分布的数据。

线性回归的优势：

- 简单性。它很容易理解，也很容易编码。

	虽然这是我第一次尝试建立线性模型，但我得到的结果只比历史上使用线性回归的最佳结果差2%。对于初学者来说，线性回归是一个好的开始。

- 效率 与其他回归方法相比，线性回归不需要花太多时间让计算机训练模型。

	一台不太老的笔记本电脑就足以应付任何种类的线性回归。

线性回归的缺点：

- 准确度。虽然85%的准确率已经足够让我们对房价有一个大致的了解，但我们肯定希望有更高的准确率，以便为我们提供一个关于数据和房屋价格之间关系的更好图像。实际上，在历史上使用的回归方法中，线性回归的准确性几乎是最差的。

- 很难找到和识别绝对最小值。如何确定成本函数J中有多少个相对最小值，这在数学上还没有得到解决。我们甚至不能知道我们得到的结果是否是最好的。

	我们改进线性模型的唯一方法是反复改变学习率和起点，希望得到一个更好的结果。

#### 多项式回归

那么，线性回归算法的缺陷是什么？答案很简单。大多数时候，X值和Y值之间的关系不是线性的。当你把图画出来时，你会发现大多数图都有曲线或转折。

这里有一个最简单的方法：用一个多项式函数而不是线性函数来拟合关系。在抛物线的例子中，我们可以用一个二次函数来描述该曲线。

![image-20221204104230412](https://blog-img-zbt.oss-cn-beijing.aliyuncs.com/picture/wuyang/202212041042441.png)

注意，每个点的x值都是一个常数，这意味着x^2也是一个常数。我们可以简单地把这种情况当作双变量线性回归，找到最适合的二次曲线，由于高斯-马尔科夫定理，我们仍然可以用平方误差作为成本函数。

正如我们所看到的，除了远离其他所有点的一个轮廓线，它的拟合效果很好。我们可以将此应用于所有单变量多项式回归的情况。

![image-20221204104321846](https://blog-img-zbt.oss-cn-beijing.aliyuncs.com/picture/wuyang/202212041043876.png)

梯度下降将比线性回归慢得多，然而，除非数据集极其巨大，否则一台普通的笔记本电脑应该可以处理它。

然而，当我们在做多项式回归时，比线性回归有更多问题。高估总是一个问题。当你让你的假设在训练数据上拟合得太好时，高估就会发生。问题是，这将使函数变得极其弯曲和混乱。例如，在图3.1的例子中，如果你不假设它是抛物线，而做一个简单的一般多项式回归，你会得到一个像这样的11度函数。

![image-20221204104418402](https://blog-img-zbt.oss-cn-beijing.aliyuncs.com/picture/wuyang/202212041044446.png)

而这肯定不是你想要的，它在a的微小变化中变化很大，这使得系统变得混乱。（不能根据a确定y，因为函数是敏感的）如何防止高估？第一个解决办法是在做模型之前先做一个假设。为什么？因为我们需要函数的程度来设定假设的方程。这在现有领域不是问题，人们已经得出了公式，只需要一些实验来证明。然而，在诸如我们的住房价格问题的情况下，这就成为一个大问题。我们能做的是通过我们的逻辑推理。例如，我们会预期一个物体的体积会直接随物体上给定长度的立方体而变化。然而，我们会期望面积和房价之间的关系接近于某种线性函数。我在这里试图表达的是，这种解决方案实际上需要我们通过一些过程来思考，这在机器学习中是不可取的。

一个更好的解决方案是在成本函数中增加一个额外的项来限制h(x)中每个项的系数和程度。对于h(x)中的每个系数，我们在成本函数中添加一个正则化系数λ。它的作用是减少转折点的数量，使曲线更加平缓。但这仍然需要时间：我们需要找到一个合适的λ来使其发挥作用。

如果λ太高，假设函数将变成线性，以最小化λ项。 如果λ太低，那么它根本无助于防止高估。这也给多项式回归的过程增加了额外的工作。

这就是多项式回归难以使用的原因。事实上，在这个房价预测案例中，多项式回归的最佳精度只比线性回归历史的最佳精度高不到1%。这可能是因为他们没有找到最佳的λ，但这实际上说明了多项式回归比它看起来要难用得多。

多项式回归的局限性：

- 尽管多项式模型通常比线性模型更适合大多数图形，但仍有一些关系是多项式不能很好处理的，如x和y之间的三角关系。
- 对异常值很敏感。如果数据集中有许多异常值，不要使用多项式回归。高阶项对离群项非常敏感，可以说比其他所有主要的回归方法都要敏感。

#### 随机森林回归

与多项式回归和线性回归非常不同，随机森林回归使用集合方法。

这意味着它实际上是许多子算法的组合，它们共同做出了很好的预测。

![image-20221204104728554](https://blog-img-zbt.oss-cn-beijing.aliyuncs.com/picture/wuyang/202212041047593.png)

现在我们知道什么是决策树了。随机森林回归是做什么的？它是决策树回归的一个改进版本。决策树回归所做的是，它形成一个像上图那样的决策树，为领域中的每个x分配一个y值。你可能想知道为什么它需要改进：它看起来相当强大和有说服力。然而它有一个致命的缺陷，即它是混乱的：x的一个小变化会导致一个完全不同的y值。在上面的例子树中，如果x=3.99，那么y=0，但如果x=4，那么y=1。0.1的变化会导致1个单位的y值变化，这不是我们想要的。一般来说，我们希望曲线更加平缓，没有跳跃。

为了解决这个问题，我们引入了随机森林回归。

它所做的是在整个训练集中随机挑选数百个样本组，并训练一个树模型来适应每个样本组。这时我们应该得到相同数量的树。为了估计某个x的y，我们只需将x放入每一棵树中，并找到h(x)s的结果的平均值。这可以防止异常值，并使 "跳跃 "更加平滑。这是目前非常普遍的回归方法。（实际上在商业领域，它比线性回归用得更多。）它通常能提供很好的准确性，但作为交换，它需要很长的时间来完成回归，因为至少要做100棵树才能做出一个很好的估计。训练树的不同方法也会花费不同的时间并给予不同的结果。有时，随机森林回归可能需要一个车间的笔记本工作时间才能得到很好的结果。

在历史上，人们用随机森林回归在房价问题上达到了90%以上的准确性。我也试过随机森林回归，虽然我使用了一个内置的方法来形成树，而且我做了100棵。最后我得到了一个体面的88%的准确率，对于像我这样没有经验的人来说，这还算不错。

以下是随机森林回归的优点和缺点：

- Advantages
	- 它通过避免使用树状方法的高估来提高准确性。
	- 它可以处理有缺失值的数据集。
	- 它对所有类型的关系都很有效，不管是线性关系还是非线性关系。
	- 它在大多数时候都能抛出异常值（但随机的意思是有时它不能）
	- 它可以解决回归和分类问题
- Disadvantage
	- 耗时：当数据库很大时，形成数百棵树是非常耗时的。
	- 基于邻居：在某些特定情况下，它的效果并不比决策树回归更好。

### Variation Methods 变形的方法

虽然大多数人使用线性回归、多项式回归或随机森林回归来解决问题，但实际上有些人使用其他方法来处理这个问题。他们使用的大多数回归方法都与前面提到的3种方法有关。下面是一些例子。

#### 岭回归

岭回归基本上是线性回归的一个变种。然而，它比线性回归更好地模拟具有多线性独立变量的数据集。多重共线性基本上是指相关的。在波士顿的房价问题上，许多独立变量都是高度相关的，比如与城市的距离和教育。因此，在这种情况下，岭回归一般比线性回归效果好。建立山脊回归模型的方法很简单。岭回归和线性回归的唯一区别是，它在线性回归的XtX上增加了一个 "常数 "矩阵项。

![image-20221204105301049](https://blog-img-zbt.oss-cn-beijing.aliyuncs.com/picture/wuyang/202212041053085.png)

基本上它可以防止模型对某一方面的考虑比其他方面多得多，这有助于提高波士顿房价问题的准确性。

#### PLS 回归

它也是线性回归的一个变种。

有时，一些因素与因变量y没有关系，或者只是轻微相关。为了提高准确性，应该将这些数据从模型中删除。我们唯一要注意的是，只有X矩阵应该被缩小。

缩小Y矩阵并没有帮助，这已经在数学上得到了证明。如果你不想要一个复杂的算法，你实际上可以分析数据的相关图，找到哪些自变量与因变量无关，并且不把它们包括在你的线性回归模型中。这与实际建立PLS模型的效果几乎一样，这也解释了为什么没有很多人使用PLS回归模型。

#### Lasso regression

拉索回归也是线性回归的一个变种。就像多项式回归一样，有时线性回归也会过拟合模型，特别是当有许多因变量，而其中一些变量的相关性不高时。除了PLS回归之外，Lasso回归也可以解决这个问题。

然而，它从另一个角度解决了这个问题。就像多项式回归一样，它增加了一个惩罚项λ

#### 决策树回归

正如我在第4节中提到的，随机森林树回归实际上是从决策树回归发展而来的。决策树回归这个词实际上指的是很多不同的树状回归方法，因为有很多方法来形成树枝。最常见的是最小平方决策树。

基本上分支的形成是为了最小化平方误差。在这个问题上，它的表现并不理想，只产生了80%左右的准确率，甚至比线性回归还差。这说明随机森林回归的效果要好得多。

### Conclusion

在本文中，讨论了三种主要的回归方法和其他三种次要的回归方法。它们的优点、缺点、局限性和应用都被清楚地列出来，以便人们轻松地检查。毋庸置疑，机器学习是我们社会的未来。尽管机器学习已经在当今社会中发挥了重要作用，但它仍然有重大缺陷，需要人们去解决。同样，没有一种回归方法是完美的。我相信他们中的大多数会被改进，而且在不久的将来会有更多的回归方法被开发出来。然而，这并不意味着我们不需要回顾过去人们如何建立回归模型。正如埃莉诺-罗斯福所说，"从别人的错误中学习。

你不可能活得足够长，自己也会犯所有的错误"。知道一个模型的缺陷是改进它的必要条件。对于那些刚接触机器学习的人来说，熟悉这些方法也有助于在需要解决问题的时候选择使用哪种方法。总之，这篇文章是你收集不同回归方法信息的绝佳来源。

另外，实际的房价问题也很重要。事实上，这些模型已经在帮助人们获得利润。

虽然这些模型不能给我们一个准确的预期，一个房子应该值多少钱（请记住，在这个问题上达到的最高准确率低于95%），但它可以说已经拯救了许多人在不公平的房屋交易中损失的钱。

最后，我希望这篇论文能帮助那些对机器学习感兴趣但不熟悉的人对机器学习产生一些兴趣。想象一下，如果机器学习能够帮助我们进行这些计算，我们的生活会变得多么简单。我们不必担心被愚弄，从日常生活到房子和汽车等大问题。

为了结束本文，我选择了戴夫-沃特的一句话，这是我最喜欢的一句话。

"计算机能够看，能够听，能够学习。 欢迎来到未来"。